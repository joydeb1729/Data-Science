{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regularization**\n",
    "\n",
    "**Regularization** is a technique used to improve the performance and generalization of machine learning models by adding a penalty term to the cost function. This helps prevent **overfitting**, where the model performs well on training data but poorly on unseen data.\n",
    "\n",
    "In regression, regularization modifies the cost function by adding a term proportional to the magnitude of the model coefficients. The two most common types of regularization are **Lasso (L1)** and **Ridge (L2)**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Use Regularization?**\n",
    "\n",
    "- **Overfitting Prevention:** Reduces the complexity of the model by shrinking the coefficients, making it less likely to overfit the data.\n",
    "- **Feature Selection:** In some regularization techniques (e.g., Lasso), certain coefficients are reduced to zero, effectively selecting only the most relevant features.\n",
    "- **Multicollinearity Handling:** Helps stabilize the model when features are highly correlated.\n",
    "\n",
    "---\n",
    "\n",
    "## **Regularized Cost Functions**\n",
    "\n",
    "In linear regression, the standard cost function is:\n",
    "$$ J(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2 $$  \n",
    "where $\\hat{y}_i = \\mathbf{w} \\cdot \\mathbf{x}_i + b$.\n",
    "\n",
    "Regularization adds a penalty term to this cost function:\n",
    "\n",
    "### 1. **Ridge Regression (L2 Regularization)**\n",
    "\n",
    "**Penalty Term:** Sum of squared coefficients  \n",
    "$$ J_{\\text{ridge}}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2 + \\lambda \\sum_{j=1}^p w_j^2 $$\n",
    "\n",
    "- $\\lambda$: Regularization parameter (controls the strength of regularization).\n",
    "- $w_j$: Coefficients of the model.\n",
    "\n",
    "**Effect of Ridge:**\n",
    "- Shrinks coefficients towards zero but does not make them exactly zero.\n",
    "- Works well when all features are relevant.\n",
    "- Helps handle multicollinearity by stabilizing coefficient estimates.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Lasso Regression (L1 Regularization)**\n",
    "\n",
    "**Penalty Term:** Sum of absolute values of coefficients  \n",
    "$$ J_{\\text{lasso}}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2 + \\lambda \\sum_{j=1}^p |w_j| $$\n",
    "\n",
    "- $\\lambda$: Regularization parameter (controls the strength of regularization).\n",
    "- $w_j$: Coefficients of the model.\n",
    "\n",
    "**Effect of Lasso:**\n",
    "- Shrinks some coefficients to exactly zero, effectively performing feature selection.\n",
    "- Useful when only a subset of features is important.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Differences Between Ridge and Lasso**\n",
    "\n",
    "| Aspect               | Ridge (L2)                     | Lasso (L1)                    |\n",
    "|----------------------|--------------------------------|-------------------------------|\n",
    "| **Penalty Term**     | $ \\sum w_j^2 $                | $ \\sum w_j $                |\n",
    "| **Coefficient Effect** | Shrinks coefficients towards zero | Can shrink coefficients to exactly zero |\n",
    "| **Feature Selection**| No                            | Yes                           |\n",
    "| **Use Case**         | When all features are relevant | When feature selection is needed |\n",
    "\n",
    "---\n",
    "\n",
    "## **Choosing $\\lambda$ (Regularization Strength)**\n",
    "\n",
    "- A higher $\\lambda$ leads to stronger regularization, shrinking coefficients more aggressively.\n",
    "- A lower $\\lambda$ results in weaker regularization, making the model closer to standard linear regression.\n",
    "- $\\lambda$ is typically chosen using techniques like **cross-validation**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Regularization and Bias-Variance Tradeoff**\n",
    "\n",
    "- Regularization increases bias but reduces variance.\n",
    "- By adding a penalty to large coefficients, the model becomes simpler and less sensitive to noise in the training data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Combined Regularization: Elastic Net**\n",
    "\n",
    "Elastic Net combines Ridge (L2) and Lasso (L1):\n",
    "$$ J_{\\text{elastic}}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2 + \\alpha \\lambda \\sum_{j=1}^p |w_j| + (1-\\alpha) \\lambda \\sum_{j=1}^p w_j^2 $$\n",
    "\n",
    "- $\\alpha$: Balances the contribution of L1 and L2 penalties.\n",
    "- Useful when dealing with correlated features and when some feature selection is desired.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "- **Ridge (L2):** Penalizes the square of coefficients, reduces their magnitude, and helps when all features are important.\n",
    "- **Lasso (L1):** Penalizes the absolute value of coefficients, can set some coefficients to zero, and performs feature selection.\n",
    "- **Elastic Net:** Combines the strengths of Ridge and Lasso for flexibility in handling different types of data.\n",
    "\n",
    "Regularization ensures the model is robust, interpretable, and capable of generalizing to new data.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
