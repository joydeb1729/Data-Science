{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Best Fit Line**\n",
    "\n",
    "A **best fit line** is a straight line that best represents the relationship between the independent variable(s) (e.g., $x$) and the dependent variable (e.g., $y$) in regression analysis. It minimizes the error between the predicted values and the actual data points.\n",
    "\n",
    "---\n",
    "\n",
    "## **How to Find the Best Fit Line**\n",
    "\n",
    "The equation of the best fit line is:  \n",
    "$$ y = mx + c $$  \n",
    "Where:\n",
    "- $m$ = slope of the line (how steep it is),\n",
    "- $c$ = y-intercept (where the line crosses the y-axis).\n",
    "\n",
    "To determine $m$ and $c$, we use optimization techniques that minimize the cost function, such as **Gradient Descent**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Gradient Descent Technique**\n",
    "\n",
    "### **Overview**\n",
    "Gradient Descent is an iterative optimization algorithm used to minimize the cost function by adjusting the model parameters ($m$ and $c$) step by step in the direction of the steepest descent.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps in Gradient Descent**\n",
    "\n",
    "1. **Initialize Parameters:**\n",
    "   - Start with initial guesses for $m$ and $c$ (e.g., $m = 0$, $c = 0$).\n",
    "\n",
    "2. **Define the Cost Function:**\n",
    "   - For linear regression, the cost function is Mean Squared Error (MSE):\n",
    "     $$ J(m, c) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - (mx_i + c) \\right)^2 $$\n",
    "\n",
    "3. **Compute the Gradients:**\n",
    "   - Partial derivatives of the cost function with respect to $m$ and $c$ are:\n",
    "     $$ \\frac{\\partial J}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i \\cdot \\left( y_i - (mx_i + c) \\right) $$\n",
    "     $$ \\frac{\\partial J}{\\partial c} = -\\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - (mx_i + c) \\right) $$\n",
    "\n",
    "4. **Update the Parameters:**\n",
    "   - Update $m$ and $c$ using the gradients and a learning rate ($\\alpha$):\n",
    "     $$ m_{\\text{new}} = m_{\\text{old}} - \\alpha \\cdot \\frac{\\partial J}{\\partial m} $$\n",
    "     $$ c_{\\text{new}} = c_{\\text{old}} - \\alpha \\cdot \\frac{\\partial J}{\\partial c} $$\n",
    "   - The learning rate determines the step size. A small $\\alpha$ slows convergence, while a large $\\alpha$ risks overshooting the minimum.\n",
    "\n",
    "5. **Iterate Until Convergence:**\n",
    "   - Repeat the update step until the cost function converges to a minimum or the change in parameters becomes negligible.\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualizing Gradient Descent**\n",
    "- **Cost Function Landscape:** \n",
    "  Gradient Descent aims to find the global minimum of the cost function (a bowl-shaped curve in linear regression).\n",
    "- **Parameter Updates:** \n",
    "  Each iteration moves $m$ and $c$ closer to the values that minimize $J(m, c)$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Gradient Descent**\n",
    "- Works well for large datasets.\n",
    "- Efficient for convex cost functions like MSE in linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "### **Variants of Gradient Descent**\n",
    "1. **Batch Gradient Descent:** \n",
    "   Uses the entire dataset to compute gradients in each iteration. It is computationally expensive for large datasets.\n",
    "2. **Stochastic Gradient Descent (SGD):** \n",
    "   Updates parameters for each data point, making it faster but more noisy.\n",
    "3. **Mini-Batch Gradient Descent:** \n",
    "   Combines the benefits of batch and SGD by updating parameters using small batches of data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "- The **best fit line** is determined by minimizing the cost function, typically MSE.\n",
    "- **Gradient Descent** optimizes the line's parameters ($m$ and $c$) iteratively by following the gradient of the cost function until convergence.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
