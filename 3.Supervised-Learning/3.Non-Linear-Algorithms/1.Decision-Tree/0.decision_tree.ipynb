{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree**\n",
    "\n",
    "A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It creates a tree-like structure where decisions are made based on feature values. \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Core Concepts**\n",
    "\n",
    "### **a. Root Node**\n",
    "- The topmost node in the tree.  \n",
    "- Represents the entire dataset and is split into child nodes based on the best attribute.\n",
    "\n",
    "### **b. Splitting**\n",
    "- The process of dividing the dataset into subsets based on a specific feature and its threshold value.  \n",
    "- Splitting continues recursively to build the tree.\n",
    "\n",
    "### **c. Decision Node**\n",
    "- A node that represents a decision to further split the dataset based on a feature value.\n",
    "\n",
    "### **d. Leaf Node**\n",
    "- The terminal node of the tree, representing the final output or class label.  \n",
    "- No further splitting occurs at this point.\n",
    "\n",
    "### **e. Branch**\n",
    "- A sub-section of the tree that connects nodes and shows the path from a parent node to its child node(s).\n",
    "\n",
    "### **f. Parent and Child Nodes**\n",
    "- **Parent Node**: A node that is split into child nodes.  \n",
    "- **Child Node**: A node derived from splitting a parent node.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Attribute Selection Measures (ASM)**\n",
    "\n",
    "Attribute selection measures are used to determine the best feature for splitting at each step. Common ASMs include:\n",
    "\n",
    "### **a. Information Gain (IG)**\n",
    "- Measures the reduction in entropy after a split.  \n",
    "- Formula for entropy:  \n",
    "  $$ \n",
    "  H(S) = -\\sum_{i=1}^n P_i \\log_2 P_i \n",
    "  $$  \n",
    "  Where $P_i$ is the probability of class $i$.  \n",
    "- Information Gain:  \n",
    "  $$ \n",
    "  IG(S, A) = H(S) - \\sum_{v \\in A} \\frac{|S_v|}{|S|} H(S_v) \n",
    "  $$  \n",
    "  Where $S_v$ is the subset after splitting on attribute $A$.\n",
    "\n",
    "- **When to Use**: Use when entropy-based splitting is required, common in ID3 and C4.5 algorithms.\n",
    "\n",
    "### **b. Gini Index**\n",
    "- Measures the impurity of a dataset. Lower values indicate a better split.  \n",
    "- Formula:  \n",
    "  $$ \n",
    "  Gini(S) = 1 - \\sum_{i=1}^n (P_i)^2 \n",
    "  $$  \n",
    "  - $P_i$: Probability of class $i$.  \n",
    "- Gini Index after a split:  \n",
    "  $$ \n",
    "  Gini_{split} = \\sum_{v \\in A} \\frac{|S_v|}{|S|} Gini(S_v) \n",
    "  $$\n",
    "\n",
    "- **When to Use**: Gini Index is faster to compute and is commonly used in CART (Classification and Regression Trees).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. CART Algorithm (Classification and Regression Trees)**\n",
    "\n",
    "The CART algorithm is the backbone of decision trees.  \n",
    "- **Steps**:\n",
    "  1. Start with the root node containing the entire dataset.\n",
    "  2. For each feature, calculate the **Gini Index** or another ASM to determine the best split.\n",
    "  3. Split the dataset at the feature and value that minimizes impurity.\n",
    "  4. Repeat the process recursively for each child node until a stopping condition is met (e.g., all samples belong to one class or maximum tree depth is reached).\n",
    "  5. Assign leaf nodes with the majority class (for classification) or mean value (for regression).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Pruning**\n",
    "\n",
    "Pruning is the process of reducing the size of a decision tree by removing nodes that do not provide significant information.  \n",
    "### **a. Types of Pruning**:\n",
    "- **Pre-Pruning**: Stop tree growth early based on a predefined condition (e.g., maximum depth).  \n",
    "- **Post-Pruning**: Grow the full tree first and then remove nodes that do not contribute significantly to the model.\n",
    "\n",
    "### **Advantages**:\n",
    "- Prevents overfitting.  \n",
    "- Improves generalization.  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Decision Tree Structure and Terms**\n",
    "\n",
    "### **Summary of Key Components**:\n",
    "- **Root Node**: Starting point with the full dataset.  \n",
    "- **Decision Nodes**: Points where the data splits based on an attribute.  \n",
    "- **Leaf Nodes**: Endpoints with a class label or regression value.  \n",
    "- **Branches**: Connections between nodes showing decisions made.  \n",
    "- **Parent and Child**: Hierarchical relationship between nodes.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Applications of Decision Trees**\n",
    "- **Classification**: Spam detection, sentiment analysis, etc.  \n",
    "- **Regression**: Predicting house prices, stock market trends, etc.\n",
    "\n",
    "Decision trees are intuitive and easy to interpret, making them widely used for various machine learning tasks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
