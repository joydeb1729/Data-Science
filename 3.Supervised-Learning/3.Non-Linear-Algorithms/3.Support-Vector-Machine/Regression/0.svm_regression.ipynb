{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Support Vector Machine (SVM) Regression**\n",
    "\n",
    "Support Vector Machine for Regression, known as **Support Vector Regression (SVR)**, is a supervised learning algorithm that builds a model to predict continuous outcomes. Unlike SVM for classification, SVR focuses on finding a function that deviates from the true target values by at most a certain threshold, called the **margin of tolerance ($\\epsilon$)**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Concepts**\n",
    "\n",
    "### **1. Objective of SVR**\n",
    "- SVR aims to find a function $ f(x) $ that predicts the target variable $ y $ within a margin of tolerance $ \\epsilon $.  \n",
    "- Unlike minimizing classification errors, SVR minimizes the magnitude of errors that exceed $ \\epsilon $ (called slack variables).  \n",
    "- The function is defined as:  \n",
    "  $ f(x) = w^T x + b $,  \n",
    "  where $ w $ is the weight vector and $ b $ is the bias.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Margin of Tolerance ($\\epsilon$)**\n",
    "- The $\\epsilon$ defines a tube around the true data points within which predictions are considered acceptable.\n",
    "- Points within the $\\epsilon$-tube incur no loss, while points outside the tube incur a loss proportional to their distance from the tube.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Slack Variables**\n",
    "- Slack variables $ \\xi $ and $ \\xi^* $ measure the deviation of predictions outside the $\\epsilon$ margin:\n",
    "  - $ \\xi $: Deviation above the margin.  \n",
    "  - $ \\xi^* $: Deviation below the margin.  \n",
    "- The goal is to minimize the total error $ (\\xi + \\xi^*) $ while maximizing the margin.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Loss Function**\n",
    "- SVR uses the **$\\epsilon$-insensitive loss function**:  \n",
    "  $ L(y, f(x)) = \\max(0, |y - f(x)| - \\epsilon) $,  \n",
    "  where $ y $ is the true value and $ f(x) $ is the predicted value.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Regularization Parameter ($C$)**\n",
    "- $ C $ controls the trade-off between margin width and the total error (slack variables).  \n",
    "  - High $ C $: Penalizes large deviations heavily, leading to a narrower margin (risk of overfitting).  \n",
    "  - Low $ C $: Allows larger deviations, resulting in a wider margin (better generalization).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Kernels in SVR**\n",
    "- Kernels transform input data into higher-dimensional spaces to capture nonlinear relationships. Common kernels include:  \n",
    "  - **Linear Kernel**: Best for linearly separable data.  \n",
    "    $ K(x, x') = x^T x' $.  \n",
    "  - **Polynomial Kernel**: Models polynomial relationships.  \n",
    "    $ K(x, x') = (\\gamma x^T x' + r)^d $.  \n",
    "  - **Gaussian RBF Kernel**: Captures complex, nonlinear relationships.  \n",
    "    $ K(x, x') = \\exp(-\\gamma ||x - x'||^2) $.  \n",
    "  - **Sigmoid Kernel**: Similar to neural network activations.  \n",
    "    $ K(x, x') = \\tanh(\\gamma x^T x' + r) $.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Hyperparameters**\n",
    "1. **$\\epsilon$ (Margin of Tolerance)**: Determines the tube size around the true values.  \n",
    "2. **$C$ (Regularization Parameter)**: Balances margin width and error minimization.  \n",
    "3. **Gamma ($\\gamma$)**: Used in kernels like RBF and polynomial to control the influence of data points.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages**\n",
    "1. Handles both linear and nonlinear regression effectively.  \n",
    "2. Robust to high-dimensional data.  \n",
    "3. Customizable via kernel selection and hyperparameter tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## **Disadvantages**\n",
    "1. Sensitive to hyperparameter choices ($\\epsilon$, $C$, $\\gamma$).  \n",
    "2. Computationally expensive for large datasets.  \n",
    "3. Requires feature scaling for effective performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Applications of SVR**\n",
    "- Predicting stock prices.  \n",
    "- Forecasting weather patterns.  \n",
    "- Modeling physical systems with continuous outcomes.  \n",
    "\n",
    "SVR is a versatile and robust regression technique, excelling in scenarios where precise control over prediction margins and error trade-offs is essential.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
