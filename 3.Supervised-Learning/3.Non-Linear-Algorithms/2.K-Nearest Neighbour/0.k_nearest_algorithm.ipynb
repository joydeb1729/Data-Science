{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **K-Nearest Neighbors (KNN)**\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a **non-parametric, instance-based learning algorithm** used for classification and regression tasks. It makes predictions based on the similarity between data points, measured using distance metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Concepts**\n",
    "\n",
    "### **1. Distance Metrics**\n",
    "KNN relies on distance measures to identify the closest neighbors to a given data point. Common distance metrics include:\n",
    "\n",
    "- **Euclidean Distance**:  \n",
    "  $ d = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $  \n",
    "  Used when the magnitude of differences between features is important. Suitable for continuous variables.\n",
    "\n",
    "- **Manhattan Distance** (L1 Norm):  \n",
    "  $ d = \\sum_{i=1}^{n} |x_i - y_i| $  \n",
    "  Measures the absolute differences along each axis. Suitable for cases where movement is constrained to grid-like paths.\n",
    "\n",
    "- **Minkowski Distance**:  \n",
    "  $ d = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{1/p} $  \n",
    "  Generalization of Euclidean and Manhattan distances. For $ p=2 $, it becomes Euclidean; for $ p=1 $, it becomes Manhattan.\n",
    "\n",
    "- **Hamming Distance**:  \n",
    "  Counts the number of differing attributes. Suitable for categorical data.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Choosing the Number of Neighbors ($k$)**\n",
    "- **Small $k$**: The model is sensitive to noise, leading to overfitting.  \n",
    "- **Large $k$**: The model generalizes better but may underfit, as the predictions rely on more distant neighbors.  \n",
    "- Optimal $k$ is often found using cross-validation.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Weighted Voting**\n",
    "- In KNN classification, each neighbor can contribute a **weighted vote** based on its distance to the target point. Closer neighbors receive higher weights:\n",
    "  $ \\text{Weight} = \\frac{1}{d} $  \n",
    "  where $ d $ is the distance to the target point.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Normalization**\n",
    "Since KNN is sensitive to feature scaling, data should be normalized or standardized to ensure that features with larger ranges do not dominate distance calculations.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages**\n",
    "1. Simple to understand and implement.  \n",
    "2. Flexible for both classification and regression tasks.  \n",
    "3. No explicit training phase (lazy learning).\n",
    "\n",
    "---\n",
    "\n",
    "## **Disadvantages**\n",
    "1. Computationally expensive during prediction for large datasets.  \n",
    "2. Sensitive to irrelevant or highly correlated features.  \n",
    "3. Requires careful choice of distance metric and $k$.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Improving KNN Performance**\n",
    "- **Feature Scaling**: Use standardization or normalization to balance feature influence.  \n",
    "- **Optimal $k$**: Use cross-validation to select the best $k$.  \n",
    "- **Weighted Voting**: Give more weight to closer neighbors.  \n",
    "- **Dimensionality Reduction**: Use techniques like PCA to reduce noise and computational cost.  \n",
    "\n",
    "---\n",
    "\n",
    "KNN is most effective for small to medium-sized datasets with well-separated classes. Choosing the appropriate distance metric, $k$, and feature preprocessing is crucial for achieving high accuracy and robust predictions.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
