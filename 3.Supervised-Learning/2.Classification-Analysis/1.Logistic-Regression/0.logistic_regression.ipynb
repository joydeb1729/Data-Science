{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**\n",
    "\n",
    "Logistic regression is a supervised learning algorithm used for classification problems. Unlike linear regression, which predicts continuous values, logistic regression predicts probabilities that map input data to discrete classes.\n",
    "\n",
    "---\n",
    "\n",
    "## **Types of Logistic Regression**\n",
    "\n",
    "1. **Binomial Logistic Regression**\n",
    "   - Handles binary classification problems where the outcome has only two possible classes.\n",
    "   - Example: Predicting whether a student passes (1) or fails (0) an exam.\n",
    "   - Output: Probability of belonging to class 1 ($P(y=1)$). If the probability is greater than a threshold (usually 0.5), the model assigns the class label 1.\n",
    "\n",
    "2. **Multinomial Logistic Regression**\n",
    "   - Used for multi-class classification problems where the outcome can belong to one of three or more categories.\n",
    "   - Example: Classifying types of flowers (setosa, versicolor, virginica).\n",
    "   - It uses the softmax function to compute the probabilities for each class.\n",
    "\n",
    "3. **Ordinal Logistic Regression**\n",
    "   - Deals with ordinal data where the outcome has categories with a natural order.\n",
    "   - Example: Predicting customer satisfaction levels (very dissatisfied, dissatisfied, neutral, satisfied, very satisfied).\n",
    "   - The model considers the order of classes while making predictions, unlike multinomial logistic regression.\n",
    "\n",
    "---\n",
    "\n",
    "## **How Logistic Regression Works**\n",
    "\n",
    "Logistic regression estimates the probability of a data point belonging to a particular class using a logistic (sigmoid) function. \n",
    "\n",
    "### 1. **Linear Combination of Features**\n",
    "The algorithm begins by calculating a linear combination of the input features:\n",
    "$$ z = m_1x_1 + m_2x_2 + m_3x_3 + \\dots + b $$\n",
    "- $m_1, m_2, \\dots$: Coefficients (weights) representing the importance of each feature.\n",
    "- $x_1, x_2, \\dots$: Input features.\n",
    "- $b$: Bias term.\n",
    "\n",
    "In regression problems, this $z$ value is used directly as the predicted output. However, in logistic regression, this linear value is passed through the **sigmoid function** to map it to a probability between 0 and 1.\n",
    "\n",
    "### 2. **Sigmoid Function**\n",
    "The sigmoid function transforms the linear regression output into a probability:\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "- $\\sigma(z)$: The predicted probability of the input belonging to the positive class (e.g., $P(y=1)$).\n",
    "- $e$: Euler's number (approximately 2.718).\n",
    "\n",
    "**Key Features of the Sigmoid Function:**\n",
    "1. **Range**: The output of the sigmoid function is always between 0 and 1, making it ideal for representing probabilities.\n",
    "2. **Interpretability**: \n",
    "   - When $z = 0$, $\\sigma(z) = 0.5$, meaning the model is equally likely to classify the input into either class.\n",
    "   - For $z > 0$, $\\sigma(z)$ approaches 1, indicating a high probability of belonging to the positive class.\n",
    "   - For $z < 0$, $\\sigma(z)$ approaches 0, indicating a high probability of belonging to the negative class.\n",
    "\n",
    "3. **Thresholding**:\n",
    "   - Logistic regression uses a threshold (commonly 0.5) to decide the class.\n",
    "   - If $\\sigma(z) \\geq 0.5$, predict class 1; otherwise, predict class 0.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Cost Function**\n",
    "Logistic regression uses the **log-loss (cross-entropy loss)** function to evaluate the error:\n",
    "$$ J(\\mathbf{w}, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) \\right] $$\n",
    "- $\\hat{y}_i = \\sigma(z_i)$: Predicted probability for instance $i$.\n",
    "- $y_i$: Actual label for instance $i$.\n",
    "\n",
    "The goal is to minimize this cost function by adjusting the weights ($\\mathbf{w}$) and bias ($b$).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Gradient Descent Optimization**\n",
    "To minimize the cost function, logistic regression uses **gradient descent**:\n",
    "- Compute gradients:\n",
    "  $$ \\frac{\\partial J}{\\partial w_j} \\text{ and } \\frac{\\partial J}{\\partial b} $$\n",
    "- Update parameters:\n",
    "  $$ w_j = w_j - \\alpha \\cdot \\frac{\\partial J}{\\partial w_j} $$  \n",
    "  $$ b = b - \\alpha \\cdot \\frac{\\partial J}{\\partial b} $$\n",
    "- $\\alpha$: Learning rate determining the step size.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Characteristics**\n",
    "1. Logistic regression provides probabilities, allowing for confidence in predictions.\n",
    "2. It assumes a linear relationship between input features and the log-odds of the outcome.\n",
    "3. Logistic regression can be extended to multinomial and ordinal problems with suitable adjustments.\n",
    "\n",
    "---\n",
    "\n",
    "## **Extensions of Logistic Regression**\n",
    "\n",
    "### **Multinomial Logistic Regression**\n",
    "- For multi-class problems, the model calculates probabilities for each class using the **softmax function**:\n",
    "  $$ P(y=i | \\mathbf{x}) = \\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}} $$\n",
    "  - $z_i$: Linear combination for class $i$.\n",
    "  - $k$: Total number of classes.\n",
    "\n",
    "### **Ordinal Logistic Regression**\n",
    "- Instead of predicting a single probability, the model predicts cumulative probabilities for ordered categories:\n",
    "  $$ P(y \\leq j | \\mathbf{x}) = \\frac{1}{1 + e^{-(\\mathbf{w} \\cdot \\mathbf{x} + b_j)}} $$\n",
    "  - Separate thresholds ($b_j$) are learned for each category.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of Logistic Regression**\n",
    "1. Simple and interpretable.\n",
    "2. Works well for linearly separable data.\n",
    "3. Computationally efficient and robust to overfitting with regularization (L1 or L2).\n",
    "\n",
    "## **Limitations**\n",
    "1. Assumes linearity between features and log-odds, which may not hold for complex datasets.\n",
    "2. Performance may degrade with multi-collinearity unless addressed.\n",
    "3. Cannot capture non-linear relationships unless extended with kernel methods.\n",
    "\n",
    "---\n",
    "\n",
    "Logistic regression remains a foundational technique for classification problems, offering simplicity, interpretability, and effectiveness for many real-world scenarios.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
