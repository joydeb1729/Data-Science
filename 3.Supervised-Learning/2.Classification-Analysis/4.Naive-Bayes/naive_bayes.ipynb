{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Naive Bayes**\n",
    "\n",
    "Naive Bayes is a family of probabilistic classification algorithms based on **Bayes' Theorem**. It assumes that all features are conditionally independent, which simplifies calculations but may not always hold in real-world datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Why \"Naive\"?**\n",
    "\n",
    "The algorithm is called \"naive\" because it makes the simplifying assumption that all features in the dataset are independent of each other, given the class label. While this assumption is rarely true in practice, the algorithm often performs well despite this limitation.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Conditional Probability**\n",
    "\n",
    "**Definition**:  \n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. It is expressed as:  \n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "Where:  \n",
    "- $P(A|B)$: Probability of event $A$ given $B$.  \n",
    "- $P(A \\cap B)$: Probability of both $A$ and $B$ occurring.  \n",
    "- $P(B)$: Probability of $B$ occurring.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Bayes' Theorem**\n",
    "\n",
    "**Definition**:  \n",
    "Bayes' Theorem relates the conditional and marginal probabilities of events. It is expressed as:  \n",
    "$$\n",
    "P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}\n",
    "$$\n",
    "Where:  \n",
    "- $P(H|E)$: Posterior probability (probability of hypothesis $H$ given evidence $E$).  \n",
    "- $P(E|H)$: Likelihood (probability of evidence $E$ given hypothesis $H$).  \n",
    "- $P(H)$: Prior probability (initial probability of hypothesis $H$).  \n",
    "- $P(E)$: Marginal probability (probability of evidence $E$).\n",
    "\n",
    "### **Interpretation**  \n",
    "Bayes' Theorem allows us to update our belief about a hypothesis as new evidence is introduced.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Types of Naive Bayes**\n",
    "\n",
    "### **a. Gaussian Naive Bayes**\n",
    "- **Description**: Assumes that the continuous features follow a Gaussian (normal) distribution.\n",
    "- **Likelihood Formula**: For a feature $x_i$, the probability is:  \n",
    "$$\n",
    "P(x_i|C) = \\frac{1}{\\sqrt{2\\pi\\sigma_C^2}} \\exp\\left(-\\frac{(x_i - \\mu_C)^2}{2\\sigma_C^2}\\right)\n",
    "$$\n",
    "  Where:\n",
    "  - $\\mu_C$: Mean of the feature for class $C$.  \n",
    "  - $\\sigma_C^2$: Variance of the feature for class $C$.\n",
    "\n",
    "- **When to Use**: Suitable for datasets with continuous numerical features that follow a normal distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### **b. Multinomial Naive Bayes**\n",
    "- **Description**: Designed for discrete count data, such as word counts in text classification.\n",
    "- **Formula**: The probability of a document $D$ belonging to class $C$ is:  \n",
    "$$\n",
    "P(C|D) \\propto P(C) \\prod_{i=1}^n P(x_i|C)\n",
    "$$\n",
    "  Where $x_i$ represents the count of the $i$-th feature.\n",
    "\n",
    "- **When to Use**: Ideal for text data or data with discrete counts, such as word frequency in document classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **c. Bernoulli Naive Bayes**\n",
    "- **Description**: Assumes binary features (presence or absence of a feature). It models data with binary outcomes.\n",
    "- **Formula**: The probability for class $C$ is:  \n",
    "$$\n",
    "P(C|X) \\propto P(C) \\prod_{i=1}^n P(x_i|C)^{x_i} \\cdot (1 - P(x_i|C))^{(1 - x_i)}\n",
    "$$\n",
    "- **When to Use**: Suitable for binary feature datasets, such as a bag-of-words model indicating the presence or absence of specific words.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. When to Use Which Naive Bayes Variant**\n",
    "\n",
    "1. **Gaussian Naive Bayes**:\n",
    "   - Use for continuous features that are approximately normally distributed.\n",
    "   - Example: Predicting outcomes based on continuous measurements like age, weight, or height.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - Use for discrete count data.\n",
    "   - Example: Text classification, such as spam detection or sentiment analysis.\n",
    "\n",
    "3. **Bernoulli Naive Bayes**:\n",
    "   - Use for binary feature data.\n",
    "   - Example: Binary document classification based on the presence or absence of specific words.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Key Strengths and Weaknesses**\n",
    "\n",
    "### **Strengths**:\n",
    "- Simple, fast, and easy to implement.\n",
    "- Works well on high-dimensional data.\n",
    "- Requires a small amount of training data.\n",
    "\n",
    "### **Weaknesses**:\n",
    "- Assumes independence among features, which may not hold in real-world data.\n",
    "- Gaussian Naive Bayes may struggle with continuous features that do not follow a normal distribution.\n",
    "\n",
    "Naive Bayes is a robust algorithm that can handle a wide range of classification problems effectively, especially when the assumptions closely align with the dataset characteristics.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
