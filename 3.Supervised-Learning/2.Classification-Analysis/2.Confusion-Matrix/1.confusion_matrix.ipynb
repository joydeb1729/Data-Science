{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Confusion Matrix**\n",
    "\n",
    "The confusion matrix is a performance evaluation metric for classification problems, providing a detailed breakdown of prediction results by comparing predicted and actual values. It is particularly useful for understanding how well a classification model performs and identifying errors.\n",
    "\n",
    "---\n",
    "\n",
    "## **Core Concepts**\n",
    "A confusion matrix is a 2D table with four primary components for a **binary classification** problem:\n",
    "\n",
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|--------------------|--------------------|\n",
    "| **Actual Positive** | True Positive (TP)       | False Negative (FN)       |\n",
    "| **Actual Negative** | False Positive (FP)      | True Negative (TN)        |\n",
    "\n",
    "### **Definitions**:\n",
    "1. **True Positive (TP)**: \n",
    "   - Cases where the model correctly predicts the positive class.\n",
    "   - Example: A cancer test correctly identifying a patient with cancer.\n",
    "2. **False Positive (FP)**:\n",
    "   - Cases where the model incorrectly predicts the positive class (Type I error).\n",
    "   - Example: A cancer test incorrectly identifying a healthy patient as having cancer.\n",
    "3. **False Negative (FN)**:\n",
    "   - Cases where the model incorrectly predicts the negative class (Type II error).\n",
    "   - Example: A cancer test incorrectly identifying a cancer patient as healthy.\n",
    "4. **True Negative (TN)**:\n",
    "   - Cases where the model correctly predicts the negative class.\n",
    "   - Example: A cancer test correctly identifying a healthy patient.\n",
    "\n",
    "---\n",
    "\n",
    "## **Performance Metrics**\n",
    "\n",
    "### **Accuracy**:\n",
    "- Measures the proportion of correct predictions out of total predictions.\n",
    "$$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "### **Error Rate**:\n",
    "- Measures the proportion of incorrect predictions out of total predictions.\n",
    "$$ \\text{Error Rate} = 1 - \\text{Accuracy} $$\n",
    "\n",
    "### **Type I Error** (False Positive Rate):\n",
    "- Probability of incorrectly predicting the positive class.\n",
    "$$ \\text{Type I Error (FP Rate)} = \\frac{FP}{FP + TN} $$\n",
    "\n",
    "### **Type II Error** (False Negative Rate):\n",
    "- Probability of incorrectly predicting the negative class.\n",
    "$$ \\text{Type II Error (FN Rate)} = \\frac{FN}{FN + TP} $$\n",
    "\n",
    "### **Precision**:\n",
    "- Fraction of true positive predictions among all predicted positive cases.\n",
    "$$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "### **Recall (Sensitivity)**:\n",
    "- Fraction of actual positive cases that are correctly predicted as positive.\n",
    "$$ \\text{Recall} = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "### **Specificity**:\n",
    "- Fraction of actual negative cases that are correctly predicted as negative.\n",
    "$$ \\text{Specificity} = \\frac{TN}{TN + FP} $$\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Insights**\n",
    "\n",
    "1. **Type I Error**:\n",
    "   - Occurs when the model falsely identifies a negative case as positive.\n",
    "   - Example: Diagnosing a healthy patient as sick.\n",
    "   - Impact: High Type I errors can lead to unnecessary treatments or interventions.\n",
    "\n",
    "2. **Type II Error**:\n",
    "   - Occurs when the model falsely identifies a positive case as negative.\n",
    "   - Example: Missing a cancer diagnosis in a sick patient.\n",
    "   - Impact: High Type II errors can lead to critical missed diagnoses or opportunities.\n",
    "\n",
    "3. **When a Model is Rejected**:\n",
    "   - A model might be rejected when it exhibits high error rates, especially if Type I or Type II errors are unacceptably high.\n",
    "   - The cost of misclassification often depends on the application. For example:\n",
    "     - In medical diagnosis, **Type II errors** (missed diagnoses) are generally more severe.\n",
    "     - In spam filtering, **Type I errors** (non-spam marked as spam) might be more tolerable.\n",
    "\n",
    "---\n",
    "\n",
    "## **Practical Example**\n",
    "\n",
    "### Scenario:\n",
    "Consider a binary classification model predicting whether a person has a disease. After training and testing:\n",
    "- The confusion matrix reveals:\n",
    "  - **True Positives (TP)**: Correctly identified diseased patients.\n",
    "  - **True Negatives (TN)**: Correctly identified healthy individuals.\n",
    "  - **False Positives (FP)**: Healthy individuals mistakenly diagnosed as diseased.\n",
    "  - **False Negatives (FN)**: Diseased patients mistakenly identified as healthy.\n",
    "\n",
    "### Example Metrics:\n",
    "- Accuracy: Proportion of correct predictions.\n",
    "- Error Rate: Proportion of incorrect predictions.\n",
    "- Precision: Focuses on the accuracy of positive predictions.\n",
    "- Recall: Emphasizes identifying all actual positive cases.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "The confusion matrix and derived metrics provide comprehensive insights into the strengths and weaknesses of a classification model. Understanding its components (TP, FP, TN, FN) is critical for evaluating and improving model performance based on the specific requirements of a task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
