{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Confusion Matrix (Sensitivity, Recall, Precision, and F1-Score)**\n",
    "\n",
    "The confusion matrix is a tool to evaluate classification models by summarizing prediction results. It provides the counts of True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN). These components help derive key metrics like **Sensitivity (Recall)**, **Precision**, and **F1-Score**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Confusion Matrix Overview**\n",
    "\n",
    "For a binary classification problem, the confusion matrix is structured as:\n",
    "\n",
    "| **Actual\\Predicted** | **Positive**        | **Negative**        |\n",
    "|-----------------------|--------------------|--------------------|\n",
    "| **Positive**          | **True Positive (TP)** | **False Negative (FN)** |\n",
    "| **Negative**          | **False Positive (FP)** | **True Negative (TN)** |\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Metrics Derived**\n",
    "\n",
    "### **1. Sensitivity (Recall)**:\n",
    "- Sensitivity, or recall, measures the ability of a model to identify all actual positive cases.\n",
    "$$ \\text{Recall (Sensitivity)} = \\frac{TP}{TP + FN} $$\n",
    "- **Interpretation**: High recall means the model effectively identifies most positive cases.\n",
    "\n",
    "#### Example:\n",
    "Imagine an email spam classifier:\n",
    "\n",
    "|                 | **Sent to Spam** | **Not Sent to Spam** |\n",
    "|-----------------|------------------|----------------------|\n",
    "| **Spam**        | TP = 80          | FN = 20             |\n",
    "| **Not Spam**    | FP = 10          | TN = 90             |\n",
    "\n",
    "- Recall:\n",
    "  $$ \\text{Recall} = \\frac{80}{80 + 20} = 0.8 \\, (80\\%) $$\n",
    "\n",
    "#### How to Increase Recall:\n",
    "- Adjust thresholds to favor positive predictions.\n",
    "- Use models less prone to false negatives, like ensemble methods.\n",
    "- Gather more diverse training data with positive cases.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Precision**:\n",
    "- Precision measures the accuracy of positive predictions.\n",
    "$$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "- **Interpretation**: High precision means most positive predictions are correct.\n",
    "\n",
    "#### Example:\n",
    "Imagine a cancer detection system:\n",
    "\n",
    "|                 | **Detected Cancer** | **Not Detected** |\n",
    "|-----------------|---------------------|------------------|\n",
    "| **Has Cancer**  | TP = 70             | FN = 30          |\n",
    "| **No Cancer**   | FP = 20             | TN = 80          |\n",
    "\n",
    "- Precision:\n",
    "  $$ \\text{Precision} = \\frac{70}{70 + 20} = 0.777 \\, (77.7\\%) $$\n",
    "\n",
    "#### How to Increase Precision:\n",
    "- Reduce false positives by using stricter thresholds.\n",
    "- Improve feature selection and preprocessing.\n",
    "- Use algorithms like Support Vector Machines or Logistic Regression with penalty tuning.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. F1-Score**:\n",
    "- F1-Score is the harmonic mean of precision and recall, balancing their trade-offs.\n",
    "$$ \\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
    "- **Interpretation**: A high F1-Score indicates good balance between precision and recall.\n",
    "\n",
    "#### Why F1-Score is Important:\n",
    "- It is ideal when dealing with imbalanced datasets.\n",
    "- For example, in cancer detection, missing cancer cases (low recall) and false alarms (low precision) are both costly.\n",
    "\n",
    "#### Example F1-Score:\n",
    "Using the above cancer example:\n",
    "- Recall:\n",
    "  $$ \\text{Recall} = \\frac{70}{70 + 30} = 0.7 \\, (70\\%) $$\n",
    "- Precision:\n",
    "  $$ \\text{Precision} = \\frac{70}{70 + 20} = 0.777 \\, (77.7\\%) $$\n",
    "- F1-Score:\n",
    "  $$ \\text{F1-Score} = 2 \\cdot \\frac{0.7 \\cdot 0.777}{0.7 + 0.777} = 0.737 \\, (73.7\\%) $$\n",
    "\n",
    "---\n",
    "\n",
    "## **Best Practices to Improve Metrics**\n",
    "\n",
    "1. **Sensitivity**:\n",
    "   - Lower the decision threshold to increase true positives.\n",
    "   - Add diverse examples of positive cases to the training dataset.\n",
    "\n",
    "2. **Precision**:\n",
    "   - Use stricter thresholds to reduce false positives.\n",
    "   - Incorporate better features that clearly separate classes.\n",
    "\n",
    "3. **F1-Score**:\n",
    "   - Balance data classes to ensure sufficient positive and negative samples.\n",
    "   - Use algorithms like Random Forest or Gradient Boosting that adapt well to imbalanced data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "The confusion matrix and its derived metrics—**Recall (Sensitivity)**, **Precision**, and **F1-Score**—offer critical insights into model performance. While recall emphasizes identifying positive cases, precision focuses on accurate predictions. The F1-Score balances these metrics, making it ideal for scenarios where both false positives and false negatives are costly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
